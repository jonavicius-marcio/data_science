2023-05-30 17:45:15,816:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-30 17:45:15,816:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-30 17:45:15,817:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-30 17:45:15,817:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-30 17:45:19,833:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-05-30 19:33:27,399:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-30 19:33:27,399:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-30 19:33:27,399:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-30 19:33:27,399:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-30 19:33:28,626:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-05-30 19:34:06,068:INFO:PyCaret RegressionExperiment
2023-05-30 19:34:06,068:INFO:Logging name: regressao01
2023-05-30 19:34:06,068:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-30 19:34:06,068:INFO:version 3.0.0
2023-05-30 19:34:06,068:INFO:Initializing setup()
2023-05-30 19:34:06,068:INFO:self.USI: 2637
2023-05-30 19:34:06,068:INFO:self._variable_keys: {'idx', 'gpu_param', 'target_param', 'transform_target_param', 'n_jobs_param', 'exp_name_log', 'fold_shuffle_param', 'data', 'memory', 'X_train', 'fold_groups_param', 'X', 'USI', 'X_test', 'logging_param', '_ml_usecase', 'html_param', 'log_plots_param', 'gpu_n_jobs_param', 'y_test', 'pipeline', 'seed', 'fold_generator', '_available_plots', 'y_train', 'exp_id', 'y'}
2023-05-30 19:34:06,068:INFO:Checking environment
2023-05-30 19:34:06,068:INFO:python_version: 3.9.13
2023-05-30 19:34:06,068:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-05-30 19:34:06,069:INFO:machine: AMD64
2023-05-30 19:34:06,069:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-30 19:34:06,069:INFO:Memory: svmem(total=33942257664, available=16827875328, percent=50.4, used=17114382336, free=16827875328)
2023-05-30 19:34:06,069:INFO:Physical Core: 6
2023-05-30 19:34:06,069:INFO:Logical Core: 12
2023-05-30 19:34:06,069:INFO:Checking libraries
2023-05-30 19:34:06,069:INFO:System:
2023-05-30 19:34:06,069:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-05-30 19:34:06,069:INFO:executable: C:\Users\Marcio Rodrigues\anaconda3\python.exe
2023-05-30 19:34:06,069:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-30 19:34:06,069:INFO:PyCaret required dependencies:
2023-05-30 19:34:06,069:INFO:                 pip: 23.1.2
2023-05-30 19:34:06,069:INFO:          setuptools: 63.4.1
2023-05-30 19:34:06,069:INFO:             pycaret: 3.0.0
2023-05-30 19:34:06,069:INFO:             IPython: 7.31.1
2023-05-30 19:34:06,069:INFO:          ipywidgets: 7.6.5
2023-05-30 19:34:06,069:INFO:                tqdm: 4.64.1
2023-05-30 19:34:06,069:INFO:               numpy: 1.21.5
2023-05-30 19:34:06,069:INFO:              pandas: 1.4.4
2023-05-30 19:34:06,069:INFO:              jinja2: 2.11.3
2023-05-30 19:34:06,069:INFO:               scipy: 1.9.1
2023-05-30 19:34:06,069:INFO:              joblib: 1.2.0
2023-05-30 19:34:06,069:INFO:             sklearn: 1.0.2
2023-05-30 19:34:06,069:INFO:                pyod: 1.0.9
2023-05-30 19:34:06,069:INFO:            imblearn: 0.10.1
2023-05-30 19:34:06,069:INFO:   category_encoders: 2.6.0
2023-05-30 19:34:06,069:INFO:            lightgbm: 3.3.5
2023-05-30 19:34:06,069:INFO:               numba: 0.55.1
2023-05-30 19:34:06,069:INFO:            requests: 2.28.1
2023-05-30 19:34:06,069:INFO:          matplotlib: 3.5.2
2023-05-30 19:34:06,069:INFO:          scikitplot: 0.3.7
2023-05-30 19:34:06,069:INFO:         yellowbrick: 1.5
2023-05-30 19:34:06,069:INFO:              plotly: 5.9.0
2023-05-30 19:34:06,069:INFO:             kaleido: 0.2.1
2023-05-30 19:34:06,069:INFO:         statsmodels: 0.13.2
2023-05-30 19:34:06,069:INFO:              sktime: 0.17.2
2023-05-30 19:34:06,069:INFO:               tbats: 1.1.3
2023-05-30 19:34:06,069:INFO:            pmdarima: 2.0.3
2023-05-30 19:34:06,069:INFO:              psutil: 5.9.0
2023-05-30 19:34:06,071:INFO:PyCaret optional dependencies:
2023-05-30 19:34:06,092:INFO:                shap: 0.41.0
2023-05-30 19:34:06,092:INFO:           interpret: Not installed
2023-05-30 19:34:06,092:INFO:                umap: Not installed
2023-05-30 19:34:06,092:INFO:    pandas_profiling: Not installed
2023-05-30 19:34:06,092:INFO:  explainerdashboard: Not installed
2023-05-30 19:34:06,092:INFO:             autoviz: Not installed
2023-05-30 19:34:06,092:INFO:           fairlearn: Not installed
2023-05-30 19:34:06,092:INFO:             xgboost: Not installed
2023-05-30 19:34:06,092:INFO:            catboost: Not installed
2023-05-30 19:34:06,092:INFO:              kmodes: Not installed
2023-05-30 19:34:06,092:INFO:             mlxtend: Not installed
2023-05-30 19:34:06,092:INFO:       statsforecast: Not installed
2023-05-30 19:34:06,092:INFO:        tune_sklearn: Not installed
2023-05-30 19:34:06,092:INFO:                 ray: Not installed
2023-05-30 19:34:06,092:INFO:            hyperopt: Not installed
2023-05-30 19:34:06,092:INFO:              optuna: Not installed
2023-05-30 19:34:06,092:INFO:               skopt: Not installed
2023-05-30 19:34:06,093:INFO:              mlflow: Not installed
2023-05-30 19:34:06,093:INFO:              gradio: Not installed
2023-05-30 19:34:06,093:INFO:             fastapi: Not installed
2023-05-30 19:34:06,093:INFO:             uvicorn: Not installed
2023-05-30 19:34:06,093:INFO:              m2cgen: Not installed
2023-05-30 19:34:06,093:INFO:           evidently: Not installed
2023-05-30 19:34:06,093:INFO:               fugue: Not installed
2023-05-30 19:34:06,093:INFO:           streamlit: Not installed
2023-05-30 19:34:06,093:INFO:             prophet: Not installed
2023-05-30 19:34:06,093:INFO:None
2023-05-30 19:34:06,093:INFO:Set up data.
2023-05-30 19:34:06,103:INFO:Set up train/test split.
2023-05-30 19:34:06,113:INFO:Set up index.
2023-05-30 19:34:06,113:INFO:Set up folding strategy.
2023-05-30 19:34:06,113:INFO:Assigning column types.
2023-05-30 19:34:06,115:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-30 19:34:06,116:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,120:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,124:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,180:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,224:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,225:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,328:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,328:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,332:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,336:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,392:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,434:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,436:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,436:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,436:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-30 19:34:06,443:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,451:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,503:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,543:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,545:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,545:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,549:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,553:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,619:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,662:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,662:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,663:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,663:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-30 19:34:06,672:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,725:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,766:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,766:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,766:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,772:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,830:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,870:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,872:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,872:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,872:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-30 19:34:06,935:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,980:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:34:06,980:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:06,981:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:07,048:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:34:07,089:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:34:07,089:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:07,089:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:07,089:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-30 19:34:07,156:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:34:07,205:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:07,206:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:07,268:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:34:07,299:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:07,299:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:07,299:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-30 19:34:07,415:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:07,415:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:07,524:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:07,524:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:07,529:INFO:Preparing preprocessing pipeline...
2023-05-30 19:34:07,529:INFO:Set up target transformation.
2023-05-30 19:34:07,529:INFO:Set up simple imputation.
2023-05-30 19:34:07,534:INFO:Set up encoding of categorical features.
2023-05-30 19:34:07,534:INFO:Set up column transformation.
2023-05-30 19:34:07,534:INFO:Set up feature normalization.
2023-05-30 19:34:07,914:INFO:Finished creating preprocessing pipeline.
2023-05-30 19:34:07,916:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\MARCIO~1\AppData\Local\Temp\joblib),
         steps=[('target_transformation',
                 TransformerWrapperWithInverse(transformer=TargetTransformer(estimator=PowerTransformer(standardize=False)))),
                ('numerical_imputer',
                 TransformerWrapper(include=['score', 'qty_stores'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 Tran...
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(include=['segment', 'sub_segment'],
                                    transformer=OneHotEncoder(cols=['segment',
                                                                    'sub_segment'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2023-05-30 19:34:07,916:INFO:Creating final display dataframe.
2023-05-30 19:34:08,254:INFO:Setup _display_container:                     Description         Value
0                    Session id           123
1                        Target  sells_volume
2                   Target type    Regression
3           Original data shape    (29692, 5)
4        Transformed data shape   (29692, 13)
5   Transformed train set shape   (20784, 13)
6    Transformed test set shape    (8908, 13)
7              Numeric features             2
8          Categorical features             2
9                    Preprocess          True
10              Imputation type        simple
11           Numeric imputation          mean
12       Categorical imputation          mode
13     Maximum one-hot encoding            25
14              Encoding method          None
15               Transformation          True
16        Transformation method   yeo-johnson
17                    Normalize          True
18             Normalize method        zscore
19             Transform target          True
20      Transform target method   yeo-johnson
21               Fold Generator         KFold
22                  Fold Number            10
23                     CPU Jobs            -1
24                      Use GPU         False
25               Log Experiment         False
26              Experiment Name   regressao01
27                          USI          2637
2023-05-30 19:34:08,383:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:08,384:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:08,487:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:08,487:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:34:08,488:INFO:setup() successfully completed in 4.26s...............
2023-05-30 19:34:08,517:INFO:Initializing compare_models()
2023-05-30 19:34:08,517:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, include=None, fold=None, round=4, cross_validation=True, sort=MAPE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MAPE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-30 19:34:08,518:INFO:Checking exceptions
2023-05-30 19:34:08,523:INFO:Preparing display monitor
2023-05-30 19:34:08,570:INFO:Initializing Linear Regression
2023-05-30 19:34:08,570:INFO:Total runtime is 0.0 minutes
2023-05-30 19:34:08,575:INFO:SubProcess create_model() called ==================================
2023-05-30 19:34:08,576:INFO:Initializing create_model()
2023-05-30 19:34:08,576:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:34:08,577:INFO:Checking exceptions
2023-05-30 19:34:08,577:INFO:Importing libraries
2023-05-30 19:34:08,578:INFO:Copying training dataset
2023-05-30 19:34:08,586:INFO:Defining folds
2023-05-30 19:34:08,586:INFO:Declaring metric variables
2023-05-30 19:34:08,591:INFO:Importing untrained model
2023-05-30 19:34:08,597:INFO:Linear Regression Imported successfully
2023-05-30 19:34:08,610:INFO:Starting cross validation
2023-05-30 19:34:08,623:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:34:34,853:INFO:Calculating mean and std
2023-05-30 19:34:34,855:INFO:Creating metrics dataframe
2023-05-30 19:34:36,336:INFO:Uploading results into container
2023-05-30 19:34:36,337:INFO:Uploading model into container now
2023-05-30 19:34:36,338:INFO:_master_model_container: 1
2023-05-30 19:34:36,338:INFO:_display_container: 2
2023-05-30 19:34:36,338:INFO:LinearRegression(n_jobs=-1)
2023-05-30 19:34:36,338:INFO:create_model() successfully completed......................................
2023-05-30 19:34:36,438:INFO:SubProcess create_model() end ==================================
2023-05-30 19:34:36,438:INFO:Creating metrics dataframe
2023-05-30 19:34:36,454:INFO:Initializing Lasso Regression
2023-05-30 19:34:36,455:INFO:Total runtime is 0.46474053859710696 minutes
2023-05-30 19:34:36,461:INFO:SubProcess create_model() called ==================================
2023-05-30 19:34:36,461:INFO:Initializing create_model()
2023-05-30 19:34:36,461:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:34:36,461:INFO:Checking exceptions
2023-05-30 19:34:36,462:INFO:Importing libraries
2023-05-30 19:34:36,462:INFO:Copying training dataset
2023-05-30 19:34:36,472:INFO:Defining folds
2023-05-30 19:34:36,473:INFO:Declaring metric variables
2023-05-30 19:34:36,481:INFO:Importing untrained model
2023-05-30 19:34:36,493:INFO:Lasso Regression Imported successfully
2023-05-30 19:34:36,506:INFO:Starting cross validation
2023-05-30 19:34:36,506:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:34:49,785:INFO:Calculating mean and std
2023-05-30 19:34:49,787:INFO:Creating metrics dataframe
2023-05-30 19:34:51,164:INFO:Uploading results into container
2023-05-30 19:34:51,165:INFO:Uploading model into container now
2023-05-30 19:34:51,166:INFO:_master_model_container: 2
2023-05-30 19:34:51,166:INFO:_display_container: 2
2023-05-30 19:34:51,166:INFO:Lasso(random_state=123)
2023-05-30 19:34:51,167:INFO:create_model() successfully completed......................................
2023-05-30 19:34:51,243:INFO:SubProcess create_model() end ==================================
2023-05-30 19:34:51,243:INFO:Creating metrics dataframe
2023-05-30 19:34:51,258:INFO:Initializing Ridge Regression
2023-05-30 19:34:51,259:INFO:Total runtime is 0.711470917860667 minutes
2023-05-30 19:34:51,264:INFO:SubProcess create_model() called ==================================
2023-05-30 19:34:51,265:INFO:Initializing create_model()
2023-05-30 19:34:51,265:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:34:51,266:INFO:Checking exceptions
2023-05-30 19:34:51,266:INFO:Importing libraries
2023-05-30 19:34:51,266:INFO:Copying training dataset
2023-05-30 19:34:51,273:INFO:Defining folds
2023-05-30 19:34:51,274:INFO:Declaring metric variables
2023-05-30 19:34:51,281:INFO:Importing untrained model
2023-05-30 19:34:51,287:INFO:Ridge Regression Imported successfully
2023-05-30 19:34:51,297:INFO:Starting cross validation
2023-05-30 19:34:51,298:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:35:03,650:INFO:Calculating mean and std
2023-05-30 19:35:03,650:INFO:Creating metrics dataframe
2023-05-30 19:35:05,067:INFO:Uploading results into container
2023-05-30 19:35:05,067:INFO:Uploading model into container now
2023-05-30 19:35:05,069:INFO:_master_model_container: 3
2023-05-30 19:35:05,069:INFO:_display_container: 2
2023-05-30 19:35:05,069:INFO:Ridge(random_state=123)
2023-05-30 19:35:05,070:INFO:create_model() successfully completed......................................
2023-05-30 19:35:05,147:INFO:SubProcess create_model() end ==================================
2023-05-30 19:35:05,147:INFO:Creating metrics dataframe
2023-05-30 19:35:05,160:INFO:Initializing Elastic Net
2023-05-30 19:35:05,161:INFO:Total runtime is 0.9431801080703736 minutes
2023-05-30 19:35:05,168:INFO:SubProcess create_model() called ==================================
2023-05-30 19:35:05,168:INFO:Initializing create_model()
2023-05-30 19:35:05,169:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:35:05,169:INFO:Checking exceptions
2023-05-30 19:35:05,169:INFO:Importing libraries
2023-05-30 19:35:05,169:INFO:Copying training dataset
2023-05-30 19:35:05,177:INFO:Defining folds
2023-05-30 19:35:05,177:INFO:Declaring metric variables
2023-05-30 19:35:05,185:INFO:Importing untrained model
2023-05-30 19:35:05,192:INFO:Elastic Net Imported successfully
2023-05-30 19:35:05,205:INFO:Starting cross validation
2023-05-30 19:35:05,206:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:35:17,517:INFO:Calculating mean and std
2023-05-30 19:35:17,517:INFO:Creating metrics dataframe
2023-05-30 19:35:18,938:INFO:Uploading results into container
2023-05-30 19:35:18,940:INFO:Uploading model into container now
2023-05-30 19:35:18,940:INFO:_master_model_container: 4
2023-05-30 19:35:18,941:INFO:_display_container: 2
2023-05-30 19:35:18,941:INFO:ElasticNet(random_state=123)
2023-05-30 19:35:18,941:INFO:create_model() successfully completed......................................
2023-05-30 19:35:19,019:INFO:SubProcess create_model() end ==================================
2023-05-30 19:35:19,019:INFO:Creating metrics dataframe
2023-05-30 19:35:19,033:INFO:Initializing Least Angle Regression
2023-05-30 19:35:19,033:INFO:Total runtime is 1.1743774930636088 minutes
2023-05-30 19:35:19,033:INFO:SubProcess create_model() called ==================================
2023-05-30 19:35:19,041:INFO:Initializing create_model()
2023-05-30 19:35:19,041:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:35:19,041:INFO:Checking exceptions
2023-05-30 19:35:19,042:INFO:Importing libraries
2023-05-30 19:35:19,042:INFO:Copying training dataset
2023-05-30 19:35:19,052:INFO:Defining folds
2023-05-30 19:35:19,053:INFO:Declaring metric variables
2023-05-30 19:35:19,060:INFO:Importing untrained model
2023-05-30 19:35:19,069:INFO:Least Angle Regression Imported successfully
2023-05-30 19:35:19,085:INFO:Starting cross validation
2023-05-30 19:35:19,087:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:35:19,491:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:19,526:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.468e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,527:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.741e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,528:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.130e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,528:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.246e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,529:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.804e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,536:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:19,555:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.637e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,556:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.462e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,556:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.032e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,556:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.852e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,557:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.852e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,560:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:19,590:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:19,590:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:19,605:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.665e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,605:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.673e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,605:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.153e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,605:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.501e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,605:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.501e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,626:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:19,631:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:19,640:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.286e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,641:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.313e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,641:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:19,641:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.433e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,645:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:19,645:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.059e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,646:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.233e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,647:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.981e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,647:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.290e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,647:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.801e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,648:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.164e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,648:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.159e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,648:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.268e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,648:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.149e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,649:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.140e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,653:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.702e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,653:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.163e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,654:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.425e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,654:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.159e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,654:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.086e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,658:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.725e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,659:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.910e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,659:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.188e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,659:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.755e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,660:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.920e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,660:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.878e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:19,661:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.877e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:35:31,526:INFO:Calculating mean and std
2023-05-30 19:35:31,528:INFO:Creating metrics dataframe
2023-05-30 19:35:32,958:INFO:Uploading results into container
2023-05-30 19:35:32,958:INFO:Uploading model into container now
2023-05-30 19:35:32,960:INFO:_master_model_container: 5
2023-05-30 19:35:32,960:INFO:_display_container: 2
2023-05-30 19:35:32,960:INFO:Lars(random_state=123)
2023-05-30 19:35:32,960:INFO:create_model() successfully completed......................................
2023-05-30 19:35:33,036:INFO:SubProcess create_model() end ==================================
2023-05-30 19:35:33,036:INFO:Creating metrics dataframe
2023-05-30 19:35:33,053:INFO:Initializing Lasso Least Angle Regression
2023-05-30 19:35:33,053:INFO:Total runtime is 1.4080409407615662 minutes
2023-05-30 19:35:33,059:INFO:SubProcess create_model() called ==================================
2023-05-30 19:35:33,059:INFO:Initializing create_model()
2023-05-30 19:35:33,059:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:35:33,059:INFO:Checking exceptions
2023-05-30 19:35:33,059:INFO:Importing libraries
2023-05-30 19:35:33,060:INFO:Copying training dataset
2023-05-30 19:35:33,068:INFO:Defining folds
2023-05-30 19:35:33,069:INFO:Declaring metric variables
2023-05-30 19:35:33,076:INFO:Importing untrained model
2023-05-30 19:35:33,084:INFO:Lasso Least Angle Regression Imported successfully
2023-05-30 19:35:33,098:INFO:Starting cross validation
2023-05-30 19:35:33,100:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:35:33,505:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:35:33,531:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:35:33,550:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:35:33,563:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:35:33,585:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:35:33,592:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:35:33,592:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:35:33,610:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:35:33,625:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:35:33,635:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:35:45,615:INFO:Calculating mean and std
2023-05-30 19:35:45,617:INFO:Creating metrics dataframe
2023-05-30 19:35:47,039:INFO:Uploading results into container
2023-05-30 19:35:47,039:INFO:Uploading model into container now
2023-05-30 19:35:47,039:INFO:_master_model_container: 6
2023-05-30 19:35:47,039:INFO:_display_container: 2
2023-05-30 19:35:47,039:INFO:LassoLars(random_state=123)
2023-05-30 19:35:47,039:INFO:create_model() successfully completed......................................
2023-05-30 19:35:47,132:INFO:SubProcess create_model() end ==================================
2023-05-30 19:35:47,132:INFO:Creating metrics dataframe
2023-05-30 19:35:47,146:INFO:Initializing Orthogonal Matching Pursuit
2023-05-30 19:35:47,146:INFO:Total runtime is 1.6429316878318787 minutes
2023-05-30 19:35:47,152:INFO:SubProcess create_model() called ==================================
2023-05-30 19:35:47,152:INFO:Initializing create_model()
2023-05-30 19:35:47,152:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:35:47,152:INFO:Checking exceptions
2023-05-30 19:35:47,152:INFO:Importing libraries
2023-05-30 19:35:47,152:INFO:Copying training dataset
2023-05-30 19:35:47,152:INFO:Defining folds
2023-05-30 19:35:47,152:INFO:Declaring metric variables
2023-05-30 19:35:47,173:INFO:Importing untrained model
2023-05-30 19:35:47,180:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-30 19:35:47,194:INFO:Starting cross validation
2023-05-30 19:35:47,195:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:35:47,611:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:47,626:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:47,633:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:47,659:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:47,669:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:47,674:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:47,701:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:47,701:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:47,722:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:47,729:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:35:59,706:INFO:Calculating mean and std
2023-05-30 19:35:59,708:INFO:Creating metrics dataframe
2023-05-30 19:36:01,139:INFO:Uploading results into container
2023-05-30 19:36:01,139:INFO:Uploading model into container now
2023-05-30 19:36:01,139:INFO:_master_model_container: 7
2023-05-30 19:36:01,154:INFO:_display_container: 2
2023-05-30 19:36:01,155:INFO:OrthogonalMatchingPursuit()
2023-05-30 19:36:01,155:INFO:create_model() successfully completed......................................
2023-05-30 19:36:01,233:INFO:SubProcess create_model() end ==================================
2023-05-30 19:36:01,233:INFO:Creating metrics dataframe
2023-05-30 19:36:01,248:INFO:Initializing Bayesian Ridge
2023-05-30 19:36:01,249:INFO:Total runtime is 1.8779649058977763 minutes
2023-05-30 19:36:01,257:INFO:SubProcess create_model() called ==================================
2023-05-30 19:36:01,257:INFO:Initializing create_model()
2023-05-30 19:36:01,257:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:36:01,257:INFO:Checking exceptions
2023-05-30 19:36:01,258:INFO:Importing libraries
2023-05-30 19:36:01,258:INFO:Copying training dataset
2023-05-30 19:36:01,268:INFO:Defining folds
2023-05-30 19:36:01,268:INFO:Declaring metric variables
2023-05-30 19:36:01,276:INFO:Importing untrained model
2023-05-30 19:36:01,284:INFO:Bayesian Ridge Imported successfully
2023-05-30 19:36:01,299:INFO:Starting cross validation
2023-05-30 19:36:01,302:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:36:13,900:INFO:Calculating mean and std
2023-05-30 19:36:13,901:INFO:Creating metrics dataframe
2023-05-30 19:36:15,359:INFO:Uploading results into container
2023-05-30 19:36:15,360:INFO:Uploading model into container now
2023-05-30 19:36:15,361:INFO:_master_model_container: 8
2023-05-30 19:36:15,361:INFO:_display_container: 2
2023-05-30 19:36:15,361:INFO:BayesianRidge()
2023-05-30 19:36:15,362:INFO:create_model() successfully completed......................................
2023-05-30 19:36:15,438:INFO:SubProcess create_model() end ==================================
2023-05-30 19:36:15,438:INFO:Creating metrics dataframe
2023-05-30 19:36:15,454:INFO:Initializing Passive Aggressive Regressor
2023-05-30 19:36:15,454:INFO:Total runtime is 2.114728546142578 minutes
2023-05-30 19:36:15,460:INFO:SubProcess create_model() called ==================================
2023-05-30 19:36:15,460:INFO:Initializing create_model()
2023-05-30 19:36:15,460:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:36:15,461:INFO:Checking exceptions
2023-05-30 19:36:15,461:INFO:Importing libraries
2023-05-30 19:36:15,461:INFO:Copying training dataset
2023-05-30 19:36:15,471:INFO:Defining folds
2023-05-30 19:36:15,471:INFO:Declaring metric variables
2023-05-30 19:36:15,479:INFO:Importing untrained model
2023-05-30 19:36:15,486:INFO:Passive Aggressive Regressor Imported successfully
2023-05-30 19:36:15,510:INFO:Starting cross validation
2023-05-30 19:36:15,512:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:36:28,136:INFO:Calculating mean and std
2023-05-30 19:36:28,136:INFO:Creating metrics dataframe
2023-05-30 19:36:29,623:INFO:Uploading results into container
2023-05-30 19:36:29,623:INFO:Uploading model into container now
2023-05-30 19:36:29,624:INFO:_master_model_container: 9
2023-05-30 19:36:29,624:INFO:_display_container: 2
2023-05-30 19:36:29,624:INFO:PassiveAggressiveRegressor(random_state=123)
2023-05-30 19:36:29,624:INFO:create_model() successfully completed......................................
2023-05-30 19:36:29,701:INFO:SubProcess create_model() end ==================================
2023-05-30 19:36:29,701:INFO:Creating metrics dataframe
2023-05-30 19:36:29,718:INFO:Initializing Huber Regressor
2023-05-30 19:36:29,718:INFO:Total runtime is 2.3524537086486816 minutes
2023-05-30 19:36:29,721:INFO:SubProcess create_model() called ==================================
2023-05-30 19:36:29,721:INFO:Initializing create_model()
2023-05-30 19:36:29,721:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:36:29,721:INFO:Checking exceptions
2023-05-30 19:36:29,721:INFO:Importing libraries
2023-05-30 19:36:29,721:INFO:Copying training dataset
2023-05-30 19:36:29,735:INFO:Defining folds
2023-05-30 19:36:29,736:INFO:Declaring metric variables
2023-05-30 19:36:29,743:INFO:Importing untrained model
2023-05-30 19:36:29,751:INFO:Huber Regressor Imported successfully
2023-05-30 19:36:29,766:INFO:Starting cross validation
2023-05-30 19:36:29,768:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:36:43,052:INFO:Calculating mean and std
2023-05-30 19:36:43,055:INFO:Creating metrics dataframe
2023-05-30 19:36:44,559:INFO:Uploading results into container
2023-05-30 19:36:44,560:INFO:Uploading model into container now
2023-05-30 19:36:44,561:INFO:_master_model_container: 10
2023-05-30 19:36:44,561:INFO:_display_container: 2
2023-05-30 19:36:44,561:INFO:HuberRegressor()
2023-05-30 19:36:44,561:INFO:create_model() successfully completed......................................
2023-05-30 19:36:44,638:INFO:SubProcess create_model() end ==================================
2023-05-30 19:36:44,638:INFO:Creating metrics dataframe
2023-05-30 19:36:44,654:INFO:Initializing K Neighbors Regressor
2023-05-30 19:36:44,654:INFO:Total runtime is 2.6013977607091268 minutes
2023-05-30 19:36:44,662:INFO:SubProcess create_model() called ==================================
2023-05-30 19:36:44,663:INFO:Initializing create_model()
2023-05-30 19:36:44,663:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:36:44,663:INFO:Checking exceptions
2023-05-30 19:36:44,664:INFO:Importing libraries
2023-05-30 19:36:44,664:INFO:Copying training dataset
2023-05-30 19:36:44,674:INFO:Defining folds
2023-05-30 19:36:44,674:INFO:Declaring metric variables
2023-05-30 19:36:44,685:INFO:Importing untrained model
2023-05-30 19:36:44,692:INFO:K Neighbors Regressor Imported successfully
2023-05-30 19:36:44,708:INFO:Starting cross validation
2023-05-30 19:36:44,710:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:36:58,168:INFO:Calculating mean and std
2023-05-30 19:36:58,170:INFO:Creating metrics dataframe
2023-05-30 19:36:59,636:INFO:Uploading results into container
2023-05-30 19:36:59,636:INFO:Uploading model into container now
2023-05-30 19:36:59,637:INFO:_master_model_container: 11
2023-05-30 19:36:59,637:INFO:_display_container: 2
2023-05-30 19:36:59,637:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-30 19:36:59,637:INFO:create_model() successfully completed......................................
2023-05-30 19:36:59,711:INFO:SubProcess create_model() end ==================================
2023-05-30 19:36:59,711:INFO:Creating metrics dataframe
2023-05-30 19:36:59,730:INFO:Initializing Decision Tree Regressor
2023-05-30 19:36:59,731:INFO:Total runtime is 2.8526694933573404 minutes
2023-05-30 19:36:59,737:INFO:SubProcess create_model() called ==================================
2023-05-30 19:36:59,738:INFO:Initializing create_model()
2023-05-30 19:36:59,738:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:36:59,738:INFO:Checking exceptions
2023-05-30 19:36:59,739:INFO:Importing libraries
2023-05-30 19:36:59,739:INFO:Copying training dataset
2023-05-30 19:36:59,746:INFO:Defining folds
2023-05-30 19:36:59,747:INFO:Declaring metric variables
2023-05-30 19:36:59,752:INFO:Importing untrained model
2023-05-30 19:36:59,757:INFO:Decision Tree Regressor Imported successfully
2023-05-30 19:36:59,771:INFO:Starting cross validation
2023-05-30 19:36:59,773:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:37:12,608:INFO:Calculating mean and std
2023-05-30 19:37:12,610:INFO:Creating metrics dataframe
2023-05-30 19:37:14,124:INFO:Uploading results into container
2023-05-30 19:37:14,125:INFO:Uploading model into container now
2023-05-30 19:37:14,126:INFO:_master_model_container: 12
2023-05-30 19:37:14,126:INFO:_display_container: 2
2023-05-30 19:37:14,126:INFO:DecisionTreeRegressor(random_state=123)
2023-05-30 19:37:14,126:INFO:create_model() successfully completed......................................
2023-05-30 19:37:14,203:INFO:SubProcess create_model() end ==================================
2023-05-30 19:37:14,204:INFO:Creating metrics dataframe
2023-05-30 19:37:14,222:INFO:Initializing Random Forest Regressor
2023-05-30 19:37:14,222:INFO:Total runtime is 3.0941885352134704 minutes
2023-05-30 19:37:14,227:INFO:SubProcess create_model() called ==================================
2023-05-30 19:37:14,227:INFO:Initializing create_model()
2023-05-30 19:37:14,227:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:37:14,227:INFO:Checking exceptions
2023-05-30 19:37:14,227:INFO:Importing libraries
2023-05-30 19:37:14,228:INFO:Copying training dataset
2023-05-30 19:37:14,236:INFO:Defining folds
2023-05-30 19:37:14,237:INFO:Declaring metric variables
2023-05-30 19:37:14,243:INFO:Importing untrained model
2023-05-30 19:37:14,249:INFO:Random Forest Regressor Imported successfully
2023-05-30 19:37:14,261:INFO:Starting cross validation
2023-05-30 19:37:14,262:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:37:19,227:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:37:19,727:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:37:19,764:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:37:33,232:INFO:Calculating mean and std
2023-05-30 19:37:33,234:INFO:Creating metrics dataframe
2023-05-30 19:37:34,718:INFO:Uploading results into container
2023-05-30 19:37:34,719:INFO:Uploading model into container now
2023-05-30 19:37:34,719:INFO:_master_model_container: 13
2023-05-30 19:37:34,719:INFO:_display_container: 2
2023-05-30 19:37:34,719:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-05-30 19:37:34,720:INFO:create_model() successfully completed......................................
2023-05-30 19:37:34,805:INFO:SubProcess create_model() end ==================================
2023-05-30 19:37:34,805:INFO:Creating metrics dataframe
2023-05-30 19:37:34,822:INFO:Initializing Extra Trees Regressor
2023-05-30 19:37:34,823:INFO:Total runtime is 3.4375462969144186 minutes
2023-05-30 19:37:34,831:INFO:SubProcess create_model() called ==================================
2023-05-30 19:37:34,832:INFO:Initializing create_model()
2023-05-30 19:37:34,832:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:37:34,833:INFO:Checking exceptions
2023-05-30 19:37:34,833:INFO:Importing libraries
2023-05-30 19:37:34,833:INFO:Copying training dataset
2023-05-30 19:37:34,844:INFO:Defining folds
2023-05-30 19:37:34,844:INFO:Declaring metric variables
2023-05-30 19:37:34,853:INFO:Importing untrained model
2023-05-30 19:37:34,862:INFO:Extra Trees Regressor Imported successfully
2023-05-30 19:37:34,882:INFO:Starting cross validation
2023-05-30 19:37:34,883:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:37:38,704:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:37:38,745:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:37:38,790:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:37:38,849:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:37:53,087:INFO:Calculating mean and std
2023-05-30 19:37:53,087:INFO:Creating metrics dataframe
2023-05-30 19:37:54,631:INFO:Uploading results into container
2023-05-30 19:37:54,632:INFO:Uploading model into container now
2023-05-30 19:37:54,633:INFO:_master_model_container: 14
2023-05-30 19:37:54,633:INFO:_display_container: 2
2023-05-30 19:37:54,634:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-05-30 19:37:54,634:INFO:create_model() successfully completed......................................
2023-05-30 19:37:54,702:INFO:SubProcess create_model() end ==================================
2023-05-30 19:37:54,702:INFO:Creating metrics dataframe
2023-05-30 19:37:54,728:INFO:Initializing AdaBoost Regressor
2023-05-30 19:37:54,729:INFO:Total runtime is 3.7693060477574667 minutes
2023-05-30 19:37:54,736:INFO:SubProcess create_model() called ==================================
2023-05-30 19:37:54,736:INFO:Initializing create_model()
2023-05-30 19:37:54,737:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:37:54,737:INFO:Checking exceptions
2023-05-30 19:37:54,737:INFO:Importing libraries
2023-05-30 19:37:54,737:INFO:Copying training dataset
2023-05-30 19:37:54,746:INFO:Defining folds
2023-05-30 19:37:54,747:INFO:Declaring metric variables
2023-05-30 19:37:54,754:INFO:Importing untrained model
2023-05-30 19:37:54,760:INFO:AdaBoost Regressor Imported successfully
2023-05-30 19:37:54,772:INFO:Starting cross validation
2023-05-30 19:37:54,774:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:38:08,547:INFO:Calculating mean and std
2023-05-30 19:38:08,549:INFO:Creating metrics dataframe
2023-05-30 19:38:10,077:INFO:Uploading results into container
2023-05-30 19:38:10,078:INFO:Uploading model into container now
2023-05-30 19:38:10,080:INFO:_master_model_container: 15
2023-05-30 19:38:10,081:INFO:_display_container: 2
2023-05-30 19:38:10,081:INFO:AdaBoostRegressor(random_state=123)
2023-05-30 19:38:10,081:INFO:create_model() successfully completed......................................
2023-05-30 19:38:10,158:INFO:SubProcess create_model() end ==================================
2023-05-30 19:38:10,158:INFO:Creating metrics dataframe
2023-05-30 19:38:10,175:INFO:Initializing Gradient Boosting Regressor
2023-05-30 19:38:10,176:INFO:Total runtime is 4.026755960782369 minutes
2023-05-30 19:38:10,181:INFO:SubProcess create_model() called ==================================
2023-05-30 19:38:10,182:INFO:Initializing create_model()
2023-05-30 19:38:10,182:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:38:10,183:INFO:Checking exceptions
2023-05-30 19:38:10,183:INFO:Importing libraries
2023-05-30 19:38:10,183:INFO:Copying training dataset
2023-05-30 19:38:10,191:INFO:Defining folds
2023-05-30 19:38:10,191:INFO:Declaring metric variables
2023-05-30 19:38:10,198:INFO:Importing untrained model
2023-05-30 19:38:10,208:INFO:Gradient Boosting Regressor Imported successfully
2023-05-30 19:38:10,219:INFO:Starting cross validation
2023-05-30 19:38:10,221:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:38:25,504:INFO:Calculating mean and std
2023-05-30 19:38:25,505:INFO:Creating metrics dataframe
2023-05-30 19:38:27,010:INFO:Uploading results into container
2023-05-30 19:38:27,011:INFO:Uploading model into container now
2023-05-30 19:38:27,011:INFO:_master_model_container: 16
2023-05-30 19:38:27,011:INFO:_display_container: 2
2023-05-30 19:38:27,011:INFO:GradientBoostingRegressor(random_state=123)
2023-05-30 19:38:27,011:INFO:create_model() successfully completed......................................
2023-05-30 19:38:27,087:INFO:SubProcess create_model() end ==================================
2023-05-30 19:38:27,088:INFO:Creating metrics dataframe
2023-05-30 19:38:27,107:INFO:Initializing Light Gradient Boosting Machine
2023-05-30 19:38:27,107:INFO:Total runtime is 4.308937700589498 minutes
2023-05-30 19:38:27,113:INFO:SubProcess create_model() called ==================================
2023-05-30 19:38:27,113:INFO:Initializing create_model()
2023-05-30 19:38:27,113:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:38:27,113:INFO:Checking exceptions
2023-05-30 19:38:27,114:INFO:Importing libraries
2023-05-30 19:38:27,114:INFO:Copying training dataset
2023-05-30 19:38:27,126:INFO:Defining folds
2023-05-30 19:38:27,126:INFO:Declaring metric variables
2023-05-30 19:38:27,135:INFO:Importing untrained model
2023-05-30 19:38:27,147:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-30 19:38:27,166:INFO:Starting cross validation
2023-05-30 19:38:27,167:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:38:43,933:INFO:Calculating mean and std
2023-05-30 19:38:43,935:INFO:Creating metrics dataframe
2023-05-30 19:38:45,477:INFO:Uploading results into container
2023-05-30 19:38:45,477:INFO:Uploading model into container now
2023-05-30 19:38:45,477:INFO:_master_model_container: 17
2023-05-30 19:38:45,477:INFO:_display_container: 2
2023-05-30 19:38:45,477:INFO:LGBMRegressor(random_state=123)
2023-05-30 19:38:45,477:INFO:create_model() successfully completed......................................
2023-05-30 19:38:45,559:INFO:SubProcess create_model() end ==================================
2023-05-30 19:38:45,559:INFO:Creating metrics dataframe
2023-05-30 19:38:45,577:INFO:Initializing Dummy Regressor
2023-05-30 19:38:45,577:INFO:Total runtime is 4.616780134042104 minutes
2023-05-30 19:38:45,583:INFO:SubProcess create_model() called ==================================
2023-05-30 19:38:45,584:INFO:Initializing create_model()
2023-05-30 19:38:45,584:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF41FE07F0>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:38:45,584:INFO:Checking exceptions
2023-05-30 19:38:45,584:INFO:Importing libraries
2023-05-30 19:38:45,585:INFO:Copying training dataset
2023-05-30 19:38:45,593:INFO:Defining folds
2023-05-30 19:38:45,593:INFO:Declaring metric variables
2023-05-30 19:38:45,598:INFO:Importing untrained model
2023-05-30 19:38:45,608:INFO:Dummy Regressor Imported successfully
2023-05-30 19:38:45,620:INFO:Starting cross validation
2023-05-30 19:38:45,622:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:38:59,007:INFO:Calculating mean and std
2023-05-30 19:38:59,009:INFO:Creating metrics dataframe
2023-05-30 19:39:00,557:INFO:Uploading results into container
2023-05-30 19:39:00,558:INFO:Uploading model into container now
2023-05-30 19:39:00,558:INFO:_master_model_container: 18
2023-05-30 19:39:00,558:INFO:_display_container: 2
2023-05-30 19:39:00,559:INFO:DummyRegressor()
2023-05-30 19:39:00,560:INFO:create_model() successfully completed......................................
2023-05-30 19:39:00,643:INFO:SubProcess create_model() end ==================================
2023-05-30 19:39:00,643:INFO:Creating metrics dataframe
2023-05-30 19:39:00,679:INFO:Initializing create_model()
2023-05-30 19:39:00,679:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF41DF5580>, estimator=AdaBoostRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:39:00,679:INFO:Checking exceptions
2023-05-30 19:39:00,681:INFO:Importing libraries
2023-05-30 19:39:00,681:INFO:Copying training dataset
2023-05-30 19:39:00,689:INFO:Defining folds
2023-05-30 19:39:00,689:INFO:Declaring metric variables
2023-05-30 19:39:00,689:INFO:Importing untrained model
2023-05-30 19:39:00,689:INFO:Declaring custom model
2023-05-30 19:39:00,690:INFO:AdaBoost Regressor Imported successfully
2023-05-30 19:39:00,691:INFO:Cross validation set to False
2023-05-30 19:39:00,691:INFO:Fitting Model
2023-05-30 19:39:02,416:INFO:AdaBoostRegressor(random_state=123)
2023-05-30 19:39:02,416:INFO:create_model() successfully completed......................................
2023-05-30 19:39:02,547:INFO:_master_model_container: 18
2023-05-30 19:39:02,547:INFO:_display_container: 2
2023-05-30 19:39:02,547:INFO:AdaBoostRegressor(random_state=123)
2023-05-30 19:39:02,548:INFO:compare_models() successfully completed......................................
2023-05-30 19:39:03,905:INFO:PyCaret RegressionExperiment
2023-05-30 19:39:03,905:INFO:Logging name: regressao01
2023-05-30 19:39:03,905:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-30 19:39:03,905:INFO:version 3.0.0
2023-05-30 19:39:03,905:INFO:Initializing setup()
2023-05-30 19:39:03,905:INFO:self.USI: 615c
2023-05-30 19:39:03,905:INFO:self._variable_keys: {'idx', 'gpu_param', 'target_param', 'transform_target_param', 'n_jobs_param', 'exp_name_log', 'fold_shuffle_param', 'data', 'memory', 'X_train', 'fold_groups_param', 'X', 'USI', 'X_test', 'logging_param', '_ml_usecase', 'html_param', 'log_plots_param', 'gpu_n_jobs_param', 'y_test', 'pipeline', 'seed', 'fold_generator', '_available_plots', 'y_train', 'exp_id', 'y'}
2023-05-30 19:39:03,905:INFO:Checking environment
2023-05-30 19:39:03,905:INFO:python_version: 3.9.13
2023-05-30 19:39:03,905:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-05-30 19:39:03,905:INFO:machine: AMD64
2023-05-30 19:39:03,905:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-30 19:39:03,905:INFO:Memory: svmem(total=33942257664, available=15518105600, percent=54.3, used=18424152064, free=15518105600)
2023-05-30 19:39:03,906:INFO:Physical Core: 6
2023-05-30 19:39:03,906:INFO:Logical Core: 12
2023-05-30 19:39:03,906:INFO:Checking libraries
2023-05-30 19:39:03,906:INFO:System:
2023-05-30 19:39:03,906:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-05-30 19:39:03,906:INFO:executable: C:\Users\Marcio Rodrigues\anaconda3\python.exe
2023-05-30 19:39:03,906:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-30 19:39:03,906:INFO:PyCaret required dependencies:
2023-05-30 19:39:03,906:INFO:                 pip: 23.1.2
2023-05-30 19:39:03,906:INFO:          setuptools: 63.4.1
2023-05-30 19:39:03,906:INFO:             pycaret: 3.0.0
2023-05-30 19:39:03,906:INFO:             IPython: 7.31.1
2023-05-30 19:39:03,906:INFO:          ipywidgets: 7.6.5
2023-05-30 19:39:03,906:INFO:                tqdm: 4.64.1
2023-05-30 19:39:03,906:INFO:               numpy: 1.21.5
2023-05-30 19:39:03,906:INFO:              pandas: 1.4.4
2023-05-30 19:39:03,906:INFO:              jinja2: 2.11.3
2023-05-30 19:39:03,906:INFO:               scipy: 1.9.1
2023-05-30 19:39:03,906:INFO:              joblib: 1.2.0
2023-05-30 19:39:03,906:INFO:             sklearn: 1.0.2
2023-05-30 19:39:03,906:INFO:                pyod: 1.0.9
2023-05-30 19:39:03,906:INFO:            imblearn: 0.10.1
2023-05-30 19:39:03,906:INFO:   category_encoders: 2.6.0
2023-05-30 19:39:03,907:INFO:            lightgbm: 3.3.5
2023-05-30 19:39:03,907:INFO:               numba: 0.55.1
2023-05-30 19:39:03,907:INFO:            requests: 2.28.1
2023-05-30 19:39:03,907:INFO:          matplotlib: 3.5.2
2023-05-30 19:39:03,907:INFO:          scikitplot: 0.3.7
2023-05-30 19:39:03,907:INFO:         yellowbrick: 1.5
2023-05-30 19:39:03,907:INFO:              plotly: 5.9.0
2023-05-30 19:39:03,907:INFO:             kaleido: 0.2.1
2023-05-30 19:39:03,907:INFO:         statsmodels: 0.13.2
2023-05-30 19:39:03,907:INFO:              sktime: 0.17.2
2023-05-30 19:39:03,907:INFO:               tbats: 1.1.3
2023-05-30 19:39:03,907:INFO:            pmdarima: 2.0.3
2023-05-30 19:39:03,907:INFO:              psutil: 5.9.0
2023-05-30 19:39:03,907:INFO:PyCaret optional dependencies:
2023-05-30 19:39:03,907:INFO:                shap: 0.41.0
2023-05-30 19:39:03,907:INFO:           interpret: Not installed
2023-05-30 19:39:03,908:INFO:                umap: Not installed
2023-05-30 19:39:03,908:INFO:    pandas_profiling: Not installed
2023-05-30 19:39:03,908:INFO:  explainerdashboard: Not installed
2023-05-30 19:39:03,908:INFO:             autoviz: Not installed
2023-05-30 19:39:03,908:INFO:           fairlearn: Not installed
2023-05-30 19:39:03,908:INFO:             xgboost: Not installed
2023-05-30 19:39:03,908:INFO:            catboost: Not installed
2023-05-30 19:39:03,908:INFO:              kmodes: Not installed
2023-05-30 19:39:03,908:INFO:             mlxtend: Not installed
2023-05-30 19:39:03,908:INFO:       statsforecast: Not installed
2023-05-30 19:39:03,908:INFO:        tune_sklearn: Not installed
2023-05-30 19:39:03,908:INFO:                 ray: Not installed
2023-05-30 19:39:03,908:INFO:            hyperopt: Not installed
2023-05-30 19:39:03,908:INFO:              optuna: Not installed
2023-05-30 19:39:03,908:INFO:               skopt: Not installed
2023-05-30 19:39:03,908:INFO:              mlflow: Not installed
2023-05-30 19:39:03,908:INFO:              gradio: Not installed
2023-05-30 19:39:03,908:INFO:             fastapi: Not installed
2023-05-30 19:39:03,908:INFO:             uvicorn: Not installed
2023-05-30 19:39:03,909:INFO:              m2cgen: Not installed
2023-05-30 19:39:03,909:INFO:           evidently: Not installed
2023-05-30 19:39:03,909:INFO:               fugue: Not installed
2023-05-30 19:39:03,909:INFO:           streamlit: Not installed
2023-05-30 19:39:03,909:INFO:             prophet: Not installed
2023-05-30 19:39:03,909:INFO:None
2023-05-30 19:39:03,909:INFO:Set up data.
2023-05-30 19:39:03,921:INFO:Set up train/test split.
2023-05-30 19:39:03,926:INFO:Set up index.
2023-05-30 19:39:03,926:INFO:Set up folding strategy.
2023-05-30 19:39:03,926:INFO:Assigning column types.
2023-05-30 19:39:03,929:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-30 19:39:03,929:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-30 19:39:03,933:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-30 19:39:03,938:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,000:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,046:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,047:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,047:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,047:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,052:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,056:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,114:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,154:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,154:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,154:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,154:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-30 19:39:04,154:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,154:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,226:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,269:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,270:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,270:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,275:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,279:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,340:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,387:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,387:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,387:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,388:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-30 19:39:04,397:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,457:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,508:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,508:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,509:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,518:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,579:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,627:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,627:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,628:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,628:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-30 19:39:04,687:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,747:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,748:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,748:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,815:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,867:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,868:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,868:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,871:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-30 19:39:04,943:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:39:04,990:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:04,991:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:05,063:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-30 19:39:05,106:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:05,106:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:05,107:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-30 19:39:05,225:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:05,225:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:05,343:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:05,343:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:05,345:INFO:Preparing preprocessing pipeline...
2023-05-30 19:39:05,345:INFO:Set up target transformation.
2023-05-30 19:39:05,345:INFO:Set up simple imputation.
2023-05-30 19:39:05,349:INFO:Set up encoding of categorical features.
2023-05-30 19:39:05,349:INFO:Set up column transformation.
2023-05-30 19:39:05,349:INFO:Set up feature normalization.
2023-05-30 19:39:05,724:INFO:Finished creating preprocessing pipeline.
2023-05-30 19:39:05,734:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\MARCIO~1\AppData\Local\Temp\joblib),
         steps=[('target_transformation',
                 TransformerWrapperWithInverse(transformer=TargetTransformer(estimator=PowerTransformer(standardize=False)))),
                ('numerical_imputer',
                 TransformerWrapper(include=['score', 'qty_stores'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 Tran...
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(include=['segment', 'sub_segment'],
                                    transformer=OneHotEncoder(cols=['segment',
                                                                    'sub_segment'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2023-05-30 19:39:05,734:INFO:Creating final display dataframe.
2023-05-30 19:39:06,089:INFO:Setup _display_container:                     Description         Value
0                    Session id           123
1                        Target  sells_volume
2                   Target type    Regression
3           Original data shape    (29692, 5)
4        Transformed data shape   (29692, 13)
5   Transformed train set shape   (20784, 13)
6    Transformed test set shape    (8908, 13)
7              Numeric features             2
8          Categorical features             2
9                    Preprocess          True
10              Imputation type        simple
11           Numeric imputation          mean
12       Categorical imputation          mode
13     Maximum one-hot encoding            25
14              Encoding method          None
15               Transformation          True
16        Transformation method   yeo-johnson
17                    Normalize          True
18             Normalize method        zscore
19             Transform target          True
20      Transform target method   yeo-johnson
21               Fold Generator         KFold
22                  Fold Number            10
23                     CPU Jobs            -1
24                      Use GPU         False
25               Log Experiment         False
26              Experiment Name   regressao01
27                          USI          615c
2023-05-30 19:39:06,219:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:06,220:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:06,336:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:06,336:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-05-30 19:39:06,337:INFO:setup() successfully completed in 3.7s...............
2023-05-30 19:39:06,357:INFO:Initializing compare_models()
2023-05-30 19:39:06,358:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, include=None, fold=None, round=4, cross_validation=True, sort=MAPE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MAPE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-30 19:39:06,358:INFO:Checking exceptions
2023-05-30 19:39:06,362:INFO:Preparing display monitor
2023-05-30 19:39:06,412:INFO:Initializing Linear Regression
2023-05-30 19:39:06,412:INFO:Total runtime is 0.0 minutes
2023-05-30 19:39:06,422:INFO:SubProcess create_model() called ==================================
2023-05-30 19:39:06,422:INFO:Initializing create_model()
2023-05-30 19:39:06,422:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:39:06,422:INFO:Checking exceptions
2023-05-30 19:39:06,424:INFO:Importing libraries
2023-05-30 19:39:06,425:INFO:Copying training dataset
2023-05-30 19:39:06,433:INFO:Defining folds
2023-05-30 19:39:06,433:INFO:Declaring metric variables
2023-05-30 19:39:06,441:INFO:Importing untrained model
2023-05-30 19:39:06,449:INFO:Linear Regression Imported successfully
2023-05-30 19:39:06,464:INFO:Starting cross validation
2023-05-30 19:39:06,465:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:39:20,515:INFO:Calculating mean and std
2023-05-30 19:39:20,517:INFO:Creating metrics dataframe
2023-05-30 19:39:22,052:INFO:Uploading results into container
2023-05-30 19:39:22,053:INFO:Uploading model into container now
2023-05-30 19:39:22,053:INFO:_master_model_container: 1
2023-05-30 19:39:22,053:INFO:_display_container: 2
2023-05-30 19:39:22,053:INFO:LinearRegression(n_jobs=-1)
2023-05-30 19:39:22,054:INFO:create_model() successfully completed......................................
2023-05-30 19:39:22,131:INFO:SubProcess create_model() end ==================================
2023-05-30 19:39:22,131:INFO:Creating metrics dataframe
2023-05-30 19:39:22,144:INFO:Initializing Lasso Regression
2023-05-30 19:39:22,145:INFO:Total runtime is 0.2622231086095174 minutes
2023-05-30 19:39:22,151:INFO:SubProcess create_model() called ==================================
2023-05-30 19:39:22,151:INFO:Initializing create_model()
2023-05-30 19:39:22,151:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:39:22,152:INFO:Checking exceptions
2023-05-30 19:39:22,152:INFO:Importing libraries
2023-05-30 19:39:22,152:INFO:Copying training dataset
2023-05-30 19:39:22,161:INFO:Defining folds
2023-05-30 19:39:22,162:INFO:Declaring metric variables
2023-05-30 19:39:22,168:INFO:Importing untrained model
2023-05-30 19:39:22,175:INFO:Lasso Regression Imported successfully
2023-05-30 19:39:22,185:INFO:Starting cross validation
2023-05-30 19:39:22,187:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:39:35,841:INFO:Calculating mean and std
2023-05-30 19:39:35,841:INFO:Creating metrics dataframe
2023-05-30 19:39:37,431:INFO:Uploading results into container
2023-05-30 19:39:37,431:INFO:Uploading model into container now
2023-05-30 19:39:37,433:INFO:_master_model_container: 2
2023-05-30 19:39:37,433:INFO:_display_container: 2
2023-05-30 19:39:37,433:INFO:Lasso(random_state=123)
2023-05-30 19:39:37,433:INFO:create_model() successfully completed......................................
2023-05-30 19:39:37,511:INFO:SubProcess create_model() end ==================================
2023-05-30 19:39:37,511:INFO:Creating metrics dataframe
2023-05-30 19:39:37,527:INFO:Initializing Ridge Regression
2023-05-30 19:39:37,528:INFO:Total runtime is 0.5186043620109558 minutes
2023-05-30 19:39:37,534:INFO:SubProcess create_model() called ==================================
2023-05-30 19:39:37,535:INFO:Initializing create_model()
2023-05-30 19:39:37,535:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:39:37,535:INFO:Checking exceptions
2023-05-30 19:39:37,535:INFO:Importing libraries
2023-05-30 19:39:37,535:INFO:Copying training dataset
2023-05-30 19:39:37,544:INFO:Defining folds
2023-05-30 19:39:37,544:INFO:Declaring metric variables
2023-05-30 19:39:37,553:INFO:Importing untrained model
2023-05-30 19:39:37,561:INFO:Ridge Regression Imported successfully
2023-05-30 19:39:37,573:INFO:Starting cross validation
2023-05-30 19:39:37,575:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:39:51,738:INFO:Calculating mean and std
2023-05-30 19:39:51,740:INFO:Creating metrics dataframe
2023-05-30 19:39:53,358:INFO:Uploading results into container
2023-05-30 19:39:53,358:INFO:Uploading model into container now
2023-05-30 19:39:53,359:INFO:_master_model_container: 3
2023-05-30 19:39:53,359:INFO:_display_container: 2
2023-05-30 19:39:53,359:INFO:Ridge(random_state=123)
2023-05-30 19:39:53,359:INFO:create_model() successfully completed......................................
2023-05-30 19:39:53,438:INFO:SubProcess create_model() end ==================================
2023-05-30 19:39:53,438:INFO:Creating metrics dataframe
2023-05-30 19:39:53,453:INFO:Initializing Elastic Net
2023-05-30 19:39:53,454:INFO:Total runtime is 0.7840451041857401 minutes
2023-05-30 19:39:53,459:INFO:SubProcess create_model() called ==================================
2023-05-30 19:39:53,460:INFO:Initializing create_model()
2023-05-30 19:39:53,460:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:39:53,460:INFO:Checking exceptions
2023-05-30 19:39:53,460:INFO:Importing libraries
2023-05-30 19:39:53,460:INFO:Copying training dataset
2023-05-30 19:39:53,469:INFO:Defining folds
2023-05-30 19:39:53,469:INFO:Declaring metric variables
2023-05-30 19:39:53,476:INFO:Importing untrained model
2023-05-30 19:39:53,483:INFO:Elastic Net Imported successfully
2023-05-30 19:39:53,496:INFO:Starting cross validation
2023-05-30 19:39:53,497:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:40:07,411:INFO:Calculating mean and std
2023-05-30 19:40:07,413:INFO:Creating metrics dataframe
2023-05-30 19:40:09,014:INFO:Uploading results into container
2023-05-30 19:40:09,015:INFO:Uploading model into container now
2023-05-30 19:40:09,015:INFO:_master_model_container: 4
2023-05-30 19:40:09,015:INFO:_display_container: 2
2023-05-30 19:40:09,016:INFO:ElasticNet(random_state=123)
2023-05-30 19:40:09,016:INFO:create_model() successfully completed......................................
2023-05-30 19:40:09,094:INFO:SubProcess create_model() end ==================================
2023-05-30 19:40:09,094:INFO:Creating metrics dataframe
2023-05-30 19:40:09,111:INFO:Initializing Least Angle Regression
2023-05-30 19:40:09,112:INFO:Total runtime is 1.0450079202651978 minutes
2023-05-30 19:40:09,121:INFO:SubProcess create_model() called ==================================
2023-05-30 19:40:09,122:INFO:Initializing create_model()
2023-05-30 19:40:09,122:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:40:09,122:INFO:Checking exceptions
2023-05-30 19:40:09,123:INFO:Importing libraries
2023-05-30 19:40:09,124:INFO:Copying training dataset
2023-05-30 19:40:09,132:INFO:Defining folds
2023-05-30 19:40:09,134:INFO:Declaring metric variables
2023-05-30 19:40:09,139:INFO:Importing untrained model
2023-05-30 19:40:09,148:INFO:Least Angle Regression Imported successfully
2023-05-30 19:40:09,164:INFO:Starting cross validation
2023-05-30 19:40:09,165:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:40:09,527:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:09,540:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.910e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,542:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.565e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,542:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.643e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,542:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.343e-07, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,542:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.396e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,542:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.168e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,542:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.107e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,577:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:09,580:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:09,591:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.320e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,591:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:09,591:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.806e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,591:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.860e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,591:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.539e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,591:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.379e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,591:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.109e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,591:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.246e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,605:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.171e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,605:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.665e-07, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,605:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.665e-07, with an active set of 8 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,615:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.837e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,617:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.534e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,617:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.530e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,618:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.302e-07, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,619:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.594e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,619:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.166e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,620:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.132e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,623:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:09,632:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.312e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,633:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.529e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,633:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.346e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,634:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.374e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,634:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.024e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,635:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.871e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,635:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.867e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,643:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:09,649:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:09,654:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.084e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,655:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.438e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,655:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.211e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,655:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.279e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,655:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.279e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,672:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:09,672:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:09,688:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.991e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,689:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.196e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,689:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.977e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,689:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.172e-07, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,690:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.816e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,691:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.586e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,691:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.584e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,696:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.866e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,696:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.963e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,697:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.688e-07, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:09,697:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.688e-07, with an active set of 8 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-30 19:40:22,785:INFO:Calculating mean and std
2023-05-30 19:40:22,787:INFO:Creating metrics dataframe
2023-05-30 19:40:24,415:INFO:Uploading results into container
2023-05-30 19:40:24,415:INFO:Uploading model into container now
2023-05-30 19:40:24,416:INFO:_master_model_container: 5
2023-05-30 19:40:24,416:INFO:_display_container: 2
2023-05-30 19:40:24,416:INFO:Lars(random_state=123)
2023-05-30 19:40:24,416:INFO:create_model() successfully completed......................................
2023-05-30 19:40:24,495:INFO:SubProcess create_model() end ==================================
2023-05-30 19:40:24,495:INFO:Creating metrics dataframe
2023-05-30 19:40:24,511:INFO:Initializing Lasso Least Angle Regression
2023-05-30 19:40:24,512:INFO:Total runtime is 1.3016697724660238 minutes
2023-05-30 19:40:24,518:INFO:SubProcess create_model() called ==================================
2023-05-30 19:40:24,518:INFO:Initializing create_model()
2023-05-30 19:40:24,518:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:40:24,518:INFO:Checking exceptions
2023-05-30 19:40:24,519:INFO:Importing libraries
2023-05-30 19:40:24,519:INFO:Copying training dataset
2023-05-30 19:40:24,527:INFO:Defining folds
2023-05-30 19:40:24,528:INFO:Declaring metric variables
2023-05-30 19:40:24,535:INFO:Importing untrained model
2023-05-30 19:40:24,542:INFO:Lasso Least Angle Regression Imported successfully
2023-05-30 19:40:24,553:INFO:Starting cross validation
2023-05-30 19:40:24,555:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:40:24,916:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:40:24,956:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:40:24,974:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:40:24,976:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:40:25,003:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:40:25,028:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:40:25,034:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:40:25,039:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:40:25,064:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:40:25,076:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-30 19:40:38,289:INFO:Calculating mean and std
2023-05-30 19:40:38,289:INFO:Creating metrics dataframe
2023-05-30 19:40:39,937:INFO:Uploading results into container
2023-05-30 19:40:39,937:INFO:Uploading model into container now
2023-05-30 19:40:39,937:INFO:_master_model_container: 6
2023-05-30 19:40:39,937:INFO:_display_container: 2
2023-05-30 19:40:39,937:INFO:LassoLars(random_state=123)
2023-05-30 19:40:39,937:INFO:create_model() successfully completed......................................
2023-05-30 19:40:40,006:INFO:SubProcess create_model() end ==================================
2023-05-30 19:40:40,006:INFO:Creating metrics dataframe
2023-05-30 19:40:40,036:INFO:Initializing Orthogonal Matching Pursuit
2023-05-30 19:40:40,036:INFO:Total runtime is 1.5604033589363098 minutes
2023-05-30 19:40:40,043:INFO:SubProcess create_model() called ==================================
2023-05-30 19:40:40,044:INFO:Initializing create_model()
2023-05-30 19:40:40,044:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:40:40,045:INFO:Checking exceptions
2023-05-30 19:40:40,045:INFO:Importing libraries
2023-05-30 19:40:40,045:INFO:Copying training dataset
2023-05-30 19:40:40,054:INFO:Defining folds
2023-05-30 19:40:40,054:INFO:Declaring metric variables
2023-05-30 19:40:40,062:INFO:Importing untrained model
2023-05-30 19:40:40,070:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-30 19:40:40,087:INFO:Starting cross validation
2023-05-30 19:40:40,089:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:40:40,449:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:40,495:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:40,506:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:40,520:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:40,542:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:40,558:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:40,569:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:40,577:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:40,605:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:40,611:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-30 19:40:54,875:INFO:Calculating mean and std
2023-05-30 19:40:54,877:INFO:Creating metrics dataframe
2023-05-30 19:40:56,510:INFO:Uploading results into container
2023-05-30 19:40:56,510:INFO:Uploading model into container now
2023-05-30 19:40:56,512:INFO:_master_model_container: 7
2023-05-30 19:40:56,512:INFO:_display_container: 2
2023-05-30 19:40:56,512:INFO:OrthogonalMatchingPursuit()
2023-05-30 19:40:56,512:INFO:create_model() successfully completed......................................
2023-05-30 19:40:56,593:INFO:SubProcess create_model() end ==================================
2023-05-30 19:40:56,593:INFO:Creating metrics dataframe
2023-05-30 19:40:56,609:INFO:Initializing Bayesian Ridge
2023-05-30 19:40:56,609:INFO:Total runtime is 1.8366191029548644 minutes
2023-05-30 19:40:56,616:INFO:SubProcess create_model() called ==================================
2023-05-30 19:40:56,617:INFO:Initializing create_model()
2023-05-30 19:40:56,617:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:40:56,617:INFO:Checking exceptions
2023-05-30 19:40:56,617:INFO:Importing libraries
2023-05-30 19:40:56,617:INFO:Copying training dataset
2023-05-30 19:40:56,625:INFO:Defining folds
2023-05-30 19:40:56,626:INFO:Declaring metric variables
2023-05-30 19:40:56,632:INFO:Importing untrained model
2023-05-30 19:40:56,638:INFO:Bayesian Ridge Imported successfully
2023-05-30 19:40:56,652:INFO:Starting cross validation
2023-05-30 19:40:56,654:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:41:10,533:INFO:Calculating mean and std
2023-05-30 19:41:10,535:INFO:Creating metrics dataframe
2023-05-30 19:41:12,122:INFO:Uploading results into container
2023-05-30 19:41:12,122:INFO:Uploading model into container now
2023-05-30 19:41:12,125:INFO:_master_model_container: 8
2023-05-30 19:41:12,125:INFO:_display_container: 2
2023-05-30 19:41:12,125:INFO:BayesianRidge()
2023-05-30 19:41:12,125:INFO:create_model() successfully completed......................................
2023-05-30 19:41:12,203:INFO:SubProcess create_model() end ==================================
2023-05-30 19:41:12,204:INFO:Creating metrics dataframe
2023-05-30 19:41:12,220:INFO:Initializing Passive Aggressive Regressor
2023-05-30 19:41:12,221:INFO:Total runtime is 2.0968168536822 minutes
2023-05-30 19:41:12,225:INFO:SubProcess create_model() called ==================================
2023-05-30 19:41:12,225:INFO:Initializing create_model()
2023-05-30 19:41:12,225:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:41:12,225:INFO:Checking exceptions
2023-05-30 19:41:12,225:INFO:Importing libraries
2023-05-30 19:41:12,225:INFO:Copying training dataset
2023-05-30 19:41:12,225:INFO:Defining folds
2023-05-30 19:41:12,225:INFO:Declaring metric variables
2023-05-30 19:41:12,245:INFO:Importing untrained model
2023-05-30 19:41:12,252:INFO:Passive Aggressive Regressor Imported successfully
2023-05-30 19:41:12,263:INFO:Starting cross validation
2023-05-30 19:41:12,263:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:41:26,209:INFO:Calculating mean and std
2023-05-30 19:41:26,211:INFO:Creating metrics dataframe
2023-05-30 19:41:27,824:INFO:Uploading results into container
2023-05-30 19:41:27,825:INFO:Uploading model into container now
2023-05-30 19:41:27,825:INFO:_master_model_container: 9
2023-05-30 19:41:27,825:INFO:_display_container: 2
2023-05-30 19:41:27,826:INFO:PassiveAggressiveRegressor(random_state=123)
2023-05-30 19:41:27,826:INFO:create_model() successfully completed......................................
2023-05-30 19:41:27,908:INFO:SubProcess create_model() end ==================================
2023-05-30 19:41:27,908:INFO:Creating metrics dataframe
2023-05-30 19:41:27,924:INFO:Initializing Huber Regressor
2023-05-30 19:41:27,925:INFO:Total runtime is 2.3585562030474345 minutes
2023-05-30 19:41:27,931:INFO:SubProcess create_model() called ==================================
2023-05-30 19:41:27,931:INFO:Initializing create_model()
2023-05-30 19:41:27,933:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:41:27,933:INFO:Checking exceptions
2023-05-30 19:41:27,933:INFO:Importing libraries
2023-05-30 19:41:27,933:INFO:Copying training dataset
2023-05-30 19:41:27,942:INFO:Defining folds
2023-05-30 19:41:27,943:INFO:Declaring metric variables
2023-05-30 19:41:27,950:INFO:Importing untrained model
2023-05-30 19:41:27,963:INFO:Huber Regressor Imported successfully
2023-05-30 19:41:27,977:INFO:Starting cross validation
2023-05-30 19:41:27,978:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:41:42,224:INFO:Calculating mean and std
2023-05-30 19:41:42,224:INFO:Creating metrics dataframe
2023-05-30 19:41:43,870:INFO:Uploading results into container
2023-05-30 19:41:43,870:INFO:Uploading model into container now
2023-05-30 19:41:43,872:INFO:_master_model_container: 10
2023-05-30 19:41:43,872:INFO:_display_container: 2
2023-05-30 19:41:43,872:INFO:HuberRegressor()
2023-05-30 19:41:43,872:INFO:create_model() successfully completed......................................
2023-05-30 19:41:43,947:INFO:SubProcess create_model() end ==================================
2023-05-30 19:41:43,947:INFO:Creating metrics dataframe
2023-05-30 19:41:43,964:INFO:Initializing K Neighbors Regressor
2023-05-30 19:41:43,965:INFO:Total runtime is 2.6258938550949096 minutes
2023-05-30 19:41:43,967:INFO:SubProcess create_model() called ==================================
2023-05-30 19:41:43,973:INFO:Initializing create_model()
2023-05-30 19:41:43,973:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:41:43,973:INFO:Checking exceptions
2023-05-30 19:41:43,973:INFO:Importing libraries
2023-05-30 19:41:43,974:INFO:Copying training dataset
2023-05-30 19:41:43,984:INFO:Defining folds
2023-05-30 19:41:43,984:INFO:Declaring metric variables
2023-05-30 19:41:43,991:INFO:Importing untrained model
2023-05-30 19:41:43,998:INFO:K Neighbors Regressor Imported successfully
2023-05-30 19:41:44,011:INFO:Starting cross validation
2023-05-30 19:41:44,012:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:41:58,781:INFO:Calculating mean and std
2023-05-30 19:41:58,783:INFO:Creating metrics dataframe
2023-05-30 19:42:00,405:INFO:Uploading results into container
2023-05-30 19:42:00,406:INFO:Uploading model into container now
2023-05-30 19:42:00,406:INFO:_master_model_container: 11
2023-05-30 19:42:00,407:INFO:_display_container: 2
2023-05-30 19:42:00,408:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-30 19:42:00,408:INFO:create_model() successfully completed......................................
2023-05-30 19:42:00,489:INFO:SubProcess create_model() end ==================================
2023-05-30 19:42:00,489:INFO:Creating metrics dataframe
2023-05-30 19:42:00,505:INFO:Initializing Decision Tree Regressor
2023-05-30 19:42:00,506:INFO:Total runtime is 2.901568897565206 minutes
2023-05-30 19:42:00,510:INFO:SubProcess create_model() called ==================================
2023-05-30 19:42:00,511:INFO:Initializing create_model()
2023-05-30 19:42:00,511:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:42:00,511:INFO:Checking exceptions
2023-05-30 19:42:00,511:INFO:Importing libraries
2023-05-30 19:42:00,511:INFO:Copying training dataset
2023-05-30 19:42:00,519:INFO:Defining folds
2023-05-30 19:42:00,519:INFO:Declaring metric variables
2023-05-30 19:42:00,526:INFO:Importing untrained model
2023-05-30 19:42:00,533:INFO:Decision Tree Regressor Imported successfully
2023-05-30 19:42:00,545:INFO:Starting cross validation
2023-05-30 19:42:00,547:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:42:14,561:INFO:Calculating mean and std
2023-05-30 19:42:14,563:INFO:Creating metrics dataframe
2023-05-30 19:42:16,208:INFO:Uploading results into container
2023-05-30 19:42:16,208:INFO:Uploading model into container now
2023-05-30 19:42:16,208:INFO:_master_model_container: 12
2023-05-30 19:42:16,208:INFO:_display_container: 2
2023-05-30 19:42:16,208:INFO:DecisionTreeRegressor(random_state=123)
2023-05-30 19:42:16,208:INFO:create_model() successfully completed......................................
2023-05-30 19:42:16,302:INFO:SubProcess create_model() end ==================================
2023-05-30 19:42:16,302:INFO:Creating metrics dataframe
2023-05-30 19:42:16,319:INFO:Initializing Random Forest Regressor
2023-05-30 19:42:16,320:INFO:Total runtime is 3.165145242214203 minutes
2023-05-30 19:42:16,326:INFO:SubProcess create_model() called ==================================
2023-05-30 19:42:16,327:INFO:Initializing create_model()
2023-05-30 19:42:16,328:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:42:16,328:INFO:Checking exceptions
2023-05-30 19:42:16,328:INFO:Importing libraries
2023-05-30 19:42:16,328:INFO:Copying training dataset
2023-05-30 19:42:16,335:INFO:Defining folds
2023-05-30 19:42:16,336:INFO:Declaring metric variables
2023-05-30 19:42:16,344:INFO:Importing untrained model
2023-05-30 19:42:16,350:INFO:Random Forest Regressor Imported successfully
2023-05-30 19:42:16,365:INFO:Starting cross validation
2023-05-30 19:42:16,366:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:42:21,452:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:42:21,773:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:42:21,927:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:42:22,089:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:42:36,537:INFO:Calculating mean and std
2023-05-30 19:42:36,539:INFO:Creating metrics dataframe
2023-05-30 19:42:38,256:INFO:Uploading results into container
2023-05-30 19:42:38,256:INFO:Uploading model into container now
2023-05-30 19:42:38,257:INFO:_master_model_container: 13
2023-05-30 19:42:38,257:INFO:_display_container: 2
2023-05-30 19:42:38,257:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-05-30 19:42:38,257:INFO:create_model() successfully completed......................................
2023-05-30 19:42:38,333:INFO:SubProcess create_model() end ==================================
2023-05-30 19:42:38,334:INFO:Creating metrics dataframe
2023-05-30 19:42:38,353:INFO:Initializing Extra Trees Regressor
2023-05-30 19:42:38,354:INFO:Total runtime is 3.5323778708775837 minutes
2023-05-30 19:42:38,360:INFO:SubProcess create_model() called ==================================
2023-05-30 19:42:38,361:INFO:Initializing create_model()
2023-05-30 19:42:38,361:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:42:38,361:INFO:Checking exceptions
2023-05-30 19:42:38,362:INFO:Importing libraries
2023-05-30 19:42:38,362:INFO:Copying training dataset
2023-05-30 19:42:38,369:INFO:Defining folds
2023-05-30 19:42:38,370:INFO:Declaring metric variables
2023-05-30 19:42:38,377:INFO:Importing untrained model
2023-05-30 19:42:38,381:INFO:Extra Trees Regressor Imported successfully
2023-05-30 19:42:38,395:INFO:Starting cross validation
2023-05-30 19:42:38,397:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:42:41,191:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:42:42,275:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:42:42,332:WARNING:C:\Users\Marcio Rodrigues\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-30 19:42:57,783:INFO:Calculating mean and std
2023-05-30 19:42:57,785:INFO:Creating metrics dataframe
2023-05-30 19:42:59,403:INFO:Uploading results into container
2023-05-30 19:42:59,404:INFO:Uploading model into container now
2023-05-30 19:42:59,404:INFO:_master_model_container: 14
2023-05-30 19:42:59,404:INFO:_display_container: 2
2023-05-30 19:42:59,405:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-05-30 19:42:59,405:INFO:create_model() successfully completed......................................
2023-05-30 19:42:59,485:INFO:SubProcess create_model() end ==================================
2023-05-30 19:42:59,485:INFO:Creating metrics dataframe
2023-05-30 19:42:59,503:INFO:Initializing AdaBoost Regressor
2023-05-30 19:42:59,504:INFO:Total runtime is 3.884870072205861 minutes
2023-05-30 19:42:59,511:INFO:SubProcess create_model() called ==================================
2023-05-30 19:42:59,512:INFO:Initializing create_model()
2023-05-30 19:42:59,512:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:42:59,512:INFO:Checking exceptions
2023-05-30 19:42:59,513:INFO:Importing libraries
2023-05-30 19:42:59,513:INFO:Copying training dataset
2023-05-30 19:42:59,522:INFO:Defining folds
2023-05-30 19:42:59,523:INFO:Declaring metric variables
2023-05-30 19:42:59,530:INFO:Importing untrained model
2023-05-30 19:42:59,537:INFO:AdaBoost Regressor Imported successfully
2023-05-30 19:42:59,552:INFO:Starting cross validation
2023-05-30 19:42:59,553:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:43:14,460:INFO:Calculating mean and std
2023-05-30 19:43:14,462:INFO:Creating metrics dataframe
2023-05-30 19:43:16,154:INFO:Uploading results into container
2023-05-30 19:43:16,155:INFO:Uploading model into container now
2023-05-30 19:43:16,155:INFO:_master_model_container: 15
2023-05-30 19:43:16,155:INFO:_display_container: 2
2023-05-30 19:43:16,156:INFO:AdaBoostRegressor(random_state=123)
2023-05-30 19:43:16,156:INFO:create_model() successfully completed......................................
2023-05-30 19:43:16,236:INFO:SubProcess create_model() end ==================================
2023-05-30 19:43:16,236:INFO:Creating metrics dataframe
2023-05-30 19:43:16,253:INFO:Initializing Gradient Boosting Regressor
2023-05-30 19:43:16,254:INFO:Total runtime is 4.164038646221161 minutes
2023-05-30 19:43:16,259:INFO:SubProcess create_model() called ==================================
2023-05-30 19:43:16,260:INFO:Initializing create_model()
2023-05-30 19:43:16,260:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:43:16,260:INFO:Checking exceptions
2023-05-30 19:43:16,261:INFO:Importing libraries
2023-05-30 19:43:16,261:INFO:Copying training dataset
2023-05-30 19:43:16,269:INFO:Defining folds
2023-05-30 19:43:16,269:INFO:Declaring metric variables
2023-05-30 19:43:16,277:INFO:Importing untrained model
2023-05-30 19:43:16,286:INFO:Gradient Boosting Regressor Imported successfully
2023-05-30 19:43:16,296:INFO:Starting cross validation
2023-05-30 19:43:16,299:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:43:32,712:INFO:Calculating mean and std
2023-05-30 19:43:32,713:INFO:Creating metrics dataframe
2023-05-30 19:43:34,406:INFO:Uploading results into container
2023-05-30 19:43:34,407:INFO:Uploading model into container now
2023-05-30 19:43:34,407:INFO:_master_model_container: 16
2023-05-30 19:43:34,407:INFO:_display_container: 2
2023-05-30 19:43:34,408:INFO:GradientBoostingRegressor(random_state=123)
2023-05-30 19:43:34,408:INFO:create_model() successfully completed......................................
2023-05-30 19:43:34,486:INFO:SubProcess create_model() end ==================================
2023-05-30 19:43:34,486:INFO:Creating metrics dataframe
2023-05-30 19:43:34,504:INFO:Initializing Light Gradient Boosting Machine
2023-05-30 19:43:34,505:INFO:Total runtime is 4.468222383658091 minutes
2023-05-30 19:43:34,511:INFO:SubProcess create_model() called ==================================
2023-05-30 19:43:34,511:INFO:Initializing create_model()
2023-05-30 19:43:34,512:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:43:34,512:INFO:Checking exceptions
2023-05-30 19:43:34,512:INFO:Importing libraries
2023-05-30 19:43:34,513:INFO:Copying training dataset
2023-05-30 19:43:34,520:INFO:Defining folds
2023-05-30 19:43:34,520:INFO:Declaring metric variables
2023-05-30 19:43:34,527:INFO:Importing untrained model
2023-05-30 19:43:34,534:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-30 19:43:34,547:INFO:Starting cross validation
2023-05-30 19:43:34,548:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:43:49,715:INFO:Calculating mean and std
2023-05-30 19:43:49,717:INFO:Creating metrics dataframe
2023-05-30 19:43:51,393:INFO:Uploading results into container
2023-05-30 19:43:51,393:INFO:Uploading model into container now
2023-05-30 19:43:51,393:INFO:_master_model_container: 17
2023-05-30 19:43:51,394:INFO:_display_container: 2
2023-05-30 19:43:51,394:INFO:LGBMRegressor(random_state=123)
2023-05-30 19:43:51,395:INFO:create_model() successfully completed......................................
2023-05-30 19:43:51,472:INFO:SubProcess create_model() end ==================================
2023-05-30 19:43:51,472:INFO:Creating metrics dataframe
2023-05-30 19:43:51,492:INFO:Initializing Dummy Regressor
2023-05-30 19:43:51,493:INFO:Total runtime is 4.7513490438461305 minutes
2023-05-30 19:43:51,499:INFO:SubProcess create_model() called ==================================
2023-05-30 19:43:51,500:INFO:Initializing create_model()
2023-05-30 19:43:51,500:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FF420B8160>, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:43:51,500:INFO:Checking exceptions
2023-05-30 19:43:51,501:INFO:Importing libraries
2023-05-30 19:43:51,501:INFO:Copying training dataset
2023-05-30 19:43:51,508:INFO:Defining folds
2023-05-30 19:43:51,509:INFO:Declaring metric variables
2023-05-30 19:43:51,514:INFO:Importing untrained model
2023-05-30 19:43:51,519:INFO:Dummy Regressor Imported successfully
2023-05-30 19:43:51,531:INFO:Starting cross validation
2023-05-30 19:43:51,532:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:44:05,877:INFO:Calculating mean and std
2023-05-30 19:44:05,895:INFO:Creating metrics dataframe
2023-05-30 19:44:07,586:INFO:Uploading results into container
2023-05-30 19:44:07,586:INFO:Uploading model into container now
2023-05-30 19:44:07,587:INFO:_master_model_container: 18
2023-05-30 19:44:07,587:INFO:_display_container: 2
2023-05-30 19:44:07,587:INFO:DummyRegressor()
2023-05-30 19:44:07,587:INFO:create_model() successfully completed......................................
2023-05-30 19:44:07,665:INFO:SubProcess create_model() end ==================================
2023-05-30 19:44:07,665:INFO:Creating metrics dataframe
2023-05-30 19:44:07,701:INFO:Initializing create_model()
2023-05-30 19:44:07,702:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=AdaBoostRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:44:07,702:INFO:Checking exceptions
2023-05-30 19:44:07,704:INFO:Importing libraries
2023-05-30 19:44:07,704:INFO:Copying training dataset
2023-05-30 19:44:07,714:INFO:Defining folds
2023-05-30 19:44:07,715:INFO:Declaring metric variables
2023-05-30 19:44:07,715:INFO:Importing untrained model
2023-05-30 19:44:07,715:INFO:Declaring custom model
2023-05-30 19:44:07,715:INFO:AdaBoost Regressor Imported successfully
2023-05-30 19:44:07,717:INFO:Cross validation set to False
2023-05-30 19:44:07,717:INFO:Fitting Model
2023-05-30 19:44:09,524:INFO:AdaBoostRegressor(random_state=123)
2023-05-30 19:44:09,524:INFO:create_model() successfully completed......................................
2023-05-30 19:44:09,652:INFO:_master_model_container: 18
2023-05-30 19:44:09,653:INFO:_display_container: 2
2023-05-30 19:44:09,653:INFO:AdaBoostRegressor(random_state=123)
2023-05-30 19:44:09,653:INFO:compare_models() successfully completed......................................
2023-05-30 19:44:55,854:INFO:Initializing create_model()
2023-05-30 19:44:55,854:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=AdaBoostRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-30 19:44:55,854:INFO:Checking exceptions
2023-05-30 19:44:55,895:INFO:Importing libraries
2023-05-30 19:44:55,896:INFO:Copying training dataset
2023-05-30 19:44:55,910:INFO:Defining folds
2023-05-30 19:44:55,910:INFO:Declaring metric variables
2023-05-30 19:44:55,910:INFO:Importing untrained model
2023-05-30 19:44:55,910:INFO:Declaring custom model
2023-05-30 19:44:55,910:INFO:AdaBoost Regressor Imported successfully
2023-05-30 19:44:55,936:INFO:Starting cross validation
2023-05-30 19:44:55,937:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-30 19:45:09,143:INFO:Calculating mean and std
2023-05-30 19:45:09,145:INFO:Creating metrics dataframe
2023-05-30 19:45:09,154:INFO:Finalizing model
2023-05-30 19:45:10,908:INFO:Uploading results into container
2023-05-30 19:45:10,909:INFO:Uploading model into container now
2023-05-30 19:45:10,923:INFO:_master_model_container: 19
2023-05-30 19:45:10,924:INFO:_display_container: 3
2023-05-30 19:45:10,924:INFO:AdaBoostRegressor(random_state=123)
2023-05-30 19:45:10,924:INFO:create_model() successfully completed......................................
2023-05-30 19:45:11,043:INFO:Initializing plot_model()
2023-05-30 19:45:11,043:INFO:plot_model(plot=feature, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=AdaBoostRegressor(random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, system=True)
2023-05-30 19:45:11,043:INFO:Checking exceptions
2023-05-30 19:45:11,055:INFO:Preloading libraries
2023-05-30 19:45:11,056:INFO:Copying training dataset
2023-05-30 19:45:11,057:INFO:Plot type: feature
2023-05-30 19:45:11,057:WARNING:No coef_ found. Trying feature_importances_
2023-05-30 19:45:11,340:INFO:Visual Rendered Successfully
2023-05-30 19:45:11,419:INFO:plot_model() successfully completed......................................
2023-05-30 19:45:11,502:INFO:Initializing predict_model()
2023-05-30 19:45:11,503:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=AdaBoostRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x000001FF424ABA60>)
2023-05-30 19:45:11,503:INFO:Checking exceptions
2023-05-30 19:45:11,503:INFO:Preloading libraries
2023-05-30 19:45:11,507:INFO:Set up data.
2023-05-30 19:45:11,528:INFO:Set up index.
2023-05-30 19:45:11,774:INFO:Initializing finalize_model()
2023-05-30 19:45:11,774:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=AdaBoostRegressor(random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-05-30 19:45:11,774:INFO:Finalizing AdaBoostRegressor(random_state=123)
2023-05-30 19:45:11,779:INFO:Initializing create_model()
2023-05-30 19:45:11,780:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=AdaBoostRegressor(random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-05-30 19:45:11,780:INFO:Checking exceptions
2023-05-30 19:45:11,782:INFO:Importing libraries
2023-05-30 19:45:11,782:INFO:Copying training dataset
2023-05-30 19:45:11,782:INFO:Defining folds
2023-05-30 19:45:11,782:INFO:Declaring metric variables
2023-05-30 19:45:11,782:INFO:Importing untrained model
2023-05-30 19:45:11,782:INFO:Declaring custom model
2023-05-30 19:45:11,783:INFO:AdaBoost Regressor Imported successfully
2023-05-30 19:45:11,784:INFO:Cross validation set to False
2023-05-30 19:45:11,784:INFO:Fitting Model
2023-05-30 19:45:12,808:INFO:Pipeline(memory=FastMemory(location=C:\Users\MARCIO~1\AppData\Local\Temp\joblib),
         steps=[('target_transformation',
                 TransformerWrapperWithInverse(transformer=TargetTransformer(estimator=PowerTransformer(standardize=False)))),
                ('numerical_imputer',
                 TransformerWrapper(include=['score', 'qty_stores'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 Tran...
                 TransformerWrapper(include=['segment', 'sub_segment'],
                                    transformer=OneHotEncoder(cols=['segment',
                                                                    'sub_segment'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator', AdaBoostRegressor(random_state=123))])
2023-05-30 19:45:12,808:INFO:create_model() successfully completed......................................
2023-05-30 19:45:12,885:INFO:_master_model_container: 19
2023-05-30 19:45:12,885:INFO:_display_container: 4
2023-05-30 19:45:12,898:INFO:Pipeline(memory=FastMemory(location=C:\Users\MARCIO~1\AppData\Local\Temp\joblib),
         steps=[('target_transformation',
                 TransformerWrapperWithInverse(transformer=TargetTransformer(estimator=PowerTransformer(standardize=False)))),
                ('numerical_imputer',
                 TransformerWrapper(include=['score', 'qty_stores'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 Tran...
                 TransformerWrapper(include=['segment', 'sub_segment'],
                                    transformer=OneHotEncoder(cols=['segment',
                                                                    'sub_segment'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator', AdaBoostRegressor(random_state=123))])
2023-05-30 19:45:12,898:INFO:finalize_model() successfully completed......................................
2023-05-30 19:45:22,752:INFO:Initializing finalize_model()
2023-05-30 19:45:22,752:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=AdaBoostRegressor(random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-05-30 19:45:22,753:INFO:Finalizing AdaBoostRegressor(random_state=123)
2023-05-30 19:45:22,757:INFO:Initializing create_model()
2023-05-30 19:45:22,757:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=AdaBoostRegressor(random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-05-30 19:45:22,758:INFO:Checking exceptions
2023-05-30 19:45:22,760:INFO:Importing libraries
2023-05-30 19:45:22,760:INFO:Copying training dataset
2023-05-30 19:45:22,761:INFO:Defining folds
2023-05-30 19:45:22,761:INFO:Declaring metric variables
2023-05-30 19:45:22,762:INFO:Importing untrained model
2023-05-30 19:45:22,762:INFO:Declaring custom model
2023-05-30 19:45:22,762:INFO:AdaBoost Regressor Imported successfully
2023-05-30 19:45:22,764:INFO:Cross validation set to False
2023-05-30 19:45:22,764:INFO:Fitting Model
2023-05-30 19:45:22,994:INFO:Pipeline(memory=FastMemory(location=C:\Users\MARCIO~1\AppData\Local\Temp\joblib),
         steps=[('target_transformation',
                 TransformerWrapperWithInverse(transformer=TargetTransformer(estimator=PowerTransformer(standardize=False)))),
                ('numerical_imputer',
                 TransformerWrapper(include=['score', 'qty_stores'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 Tran...
                 TransformerWrapper(include=['segment', 'sub_segment'],
                                    transformer=OneHotEncoder(cols=['segment',
                                                                    'sub_segment'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator', AdaBoostRegressor(random_state=123))])
2023-05-30 19:45:22,994:INFO:create_model() successfully completed......................................
2023-05-30 19:45:23,069:INFO:_master_model_container: 19
2023-05-30 19:45:23,069:INFO:_display_container: 4
2023-05-30 19:45:23,083:INFO:Pipeline(memory=FastMemory(location=C:\Users\MARCIO~1\AppData\Local\Temp\joblib),
         steps=[('target_transformation',
                 TransformerWrapperWithInverse(transformer=TargetTransformer(estimator=PowerTransformer(standardize=False)))),
                ('numerical_imputer',
                 TransformerWrapper(include=['score', 'qty_stores'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 Tran...
                 TransformerWrapper(include=['segment', 'sub_segment'],
                                    transformer=OneHotEncoder(cols=['segment',
                                                                    'sub_segment'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator', AdaBoostRegressor(random_state=123))])
2023-05-30 19:45:23,083:INFO:finalize_model() successfully completed......................................
2023-05-30 19:45:28,416:INFO:Initializing save_model()
2023-05-30 19:45:28,416:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\MARCIO~1\AppData\Local\Temp\joblib),
         steps=[('target_transformation',
                 TransformerWrapperWithInverse(transformer=TargetTransformer(estimator=PowerTransformer(standardize=False)))),
                ('numerical_imputer',
                 TransformerWrapper(include=['score', 'qty_stores'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 Tran...
                 TransformerWrapper(include=['segment', 'sub_segment'],
                                    transformer=OneHotEncoder(cols=['segment',
                                                                    'sub_segment'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator', AdaBoostRegressor(random_state=123))]), model_name=model_final, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\MARCIO~1\AppData\Local\Temp\joblib),
         steps=[('target_transformation',
                 TransformerWrapperWithInverse(transformer=TargetTransformer(estimator=PowerTransformer(standardize=False)))),
                ('numerical_imputer',
                 TransformerWrapper(include=['score', 'qty_stores'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 Tran...
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(include=['segment', 'sub_segment'],
                                    transformer=OneHotEncoder(cols=['segment',
                                                                    'sub_segment'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2023-05-30 19:45:28,416:INFO:Adding model into prep_pipe
2023-05-30 19:45:28,418:WARNING:Only Model saved as it was a pipeline.
2023-05-30 19:45:28,432:INFO:model_final.pkl saved in current working directory
2023-05-30 19:45:28,443:INFO:Pipeline(memory=FastMemory(location=C:\Users\MARCIO~1\AppData\Local\Temp\joblib),
         steps=[('target_transformation',
                 TransformerWrapperWithInverse(transformer=TargetTransformer(estimator=PowerTransformer(standardize=False)))),
                ('numerical_imputer',
                 TransformerWrapper(include=['score', 'qty_stores'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 Tran...
                 TransformerWrapper(include=['segment', 'sub_segment'],
                                    transformer=OneHotEncoder(cols=['segment',
                                                                    'sub_segment'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator', AdaBoostRegressor(random_state=123))])
2023-05-30 19:45:28,443:INFO:save_model() successfully completed......................................
2023-05-30 19:45:34,669:INFO:Initializing load_model()
2023-05-30 19:45:34,670:INFO:load_model(model_name=model_brazil_in_home, platform=None, authentication=None, verbose=True)
2023-05-30 19:45:40,625:INFO:Initializing load_model()
2023-05-30 19:45:40,625:INFO:load_model(model_name=model_final, platform=None, authentication=None, verbose=True)
2023-05-30 19:45:42,303:INFO:Initializing predict_model()
2023-05-30 19:45:42,304:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001FF421C6490>, estimator=Pipeline(memory=FastMemory(location=C:\Users\MARCIO~1\AppData\Local\Temp\joblib),
         steps=[('target_transformation',
                 TransformerWrapperWithInverse(transformer=TargetTransformer(estimator=PowerTransformer(standardize=False)))),
                ('numerical_imputer',
                 TransformerWrapper(include=['score', 'qty_stores'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 Tran...
                 TransformerWrapper(include=['segment', 'sub_segment'],
                                    transformer=OneHotEncoder(cols=['segment',
                                                                    'sub_segment'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator', AdaBoostRegressor(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x000001FF424AB820>)
2023-05-30 19:45:42,304:INFO:Checking exceptions
2023-05-30 19:45:42,304:INFO:Preloading libraries
2023-05-30 19:45:42,309:INFO:Set up data.
2023-05-30 19:45:42,322:INFO:Set up index.
